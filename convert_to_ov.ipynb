{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = \"vitb\"\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "}\n",
    "\n",
    "weights_path = f\"weights/depth_anything_v2_{model_select}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DepthAnythingV2(**model_configs[model_select]).eval()\n",
    "model.load_state_dict(torch.load(weights_path, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://images.pexels.com/photos/5740792/pexels-photo-5740792.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\"\n",
    "image = np.array(utils.download_image(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pytorch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(image):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            image: RGB image, [Height, Width, Channels] as numpy array\n",
    "        Output:\n",
    "            input_tensor, (h_o, w_o)\n",
    "        input_tensor -> ready to feed the model\n",
    "        and original height and width of the given image\n",
    "    \"\"\"\n",
    "    # save original shape\n",
    "    image_size = image.shape[:2]\n",
    "    # normalize [0, 1]\n",
    "    input_tensor = image / 255.0\n",
    "    # Resize to [518, 518]\n",
    "    input_tensor = cv2.resize(input_tensor, dsize=[518, 518], interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # mean and std\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "    input_tensor = (input_tensor - mean) / std\n",
    "\n",
    "    # turn it channels first.\n",
    "    # (h, w, c) -> (c, h, w)\n",
    "    input_tensor = np.transpose(input_tensor, (2, 0, 1))\n",
    "\n",
    "    # add batch size\n",
    "    input_tensor = np.expand_dims(input_tensor, 0)\n",
    "\n",
    "    # force dtype to float32\n",
    "    input_tensor = input_tensor.astype(\"float32\")\n",
    "    return input_tensor, image_size\n",
    "\n",
    "def postprocess(model_output, image_size):\n",
    "    depth = model_output.cpu().detach().numpy()[0]\n",
    "    h, w = image_size\n",
    "    depth = cv2.resize(depth, dsize=(w, h), interpolation=cv2.INTER_AREA)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, image_size = image_preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = model.forward(torch.from_numpy(input_tensor))\n",
    "depth = postprocess(depth, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to OpenVINO IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "ov_model_path = Path(\"models_ov\") / Path(Path(weights_path).name.replace(\".pth\", \".xml\"))\n",
    "if not ov_model_path.exists():\n",
    "    ov_model = ov.convert_model(model, example_input=input_tensor, input=[1, 3, 518, 518])\n",
    "    ov.save_model(ov_model, ov_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastdepthv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
